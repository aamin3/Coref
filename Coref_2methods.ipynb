{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coref_2methods.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SSG7Obi47x_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\n",
        "!unzip stanford-corenlp-full-2018-10-05.zip\n",
        "!pip install stanfordcorenlp\n",
        "!pip install https://github.com/huggingface/neuralcoref-models/releases/download/en_coref_sm-3.0.0/en_coref_sm-3.0.0.tar.gz\n",
        "!pip install -y spacy==2.0.12\n",
        "!pip install cymem==1.31.2\n",
        "!pip install neuralcoref --no-binary neuralcoref\n",
        "!pip install word2number"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "513OsSBiPBXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install textacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0s8iGvD6AVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import en_coref_sm\n",
        "from stanfordcorenlp import StanfordCoreNLP\n",
        "from nltk.parse.corenlp import CoreNLPParser\n",
        "import visualization\n",
        "\n",
        "STANFORD_CORENLP_PATH = 'stanford-corenlp-full-2018-10-05/'\n",
        "PORT = 9090\n",
        "try:\n",
        "    server = StanfordCoreNLP(STANFORD_CORENLP_PATH, port=PORT, quiet=True)\n",
        "except OSError as e:\n",
        "    print('The port is occupied, probably an instance is already running.')\n",
        "    server = StanfordCoreNLP('http://localhost', port=PORT, quiet=True)\n",
        "    \n",
        "STANFORD_SERVER_URL = server.url"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3twpByE_6ME1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create model \n",
        "import spacy\n",
        "import en_coref_sm\n",
        "\n",
        "stanford_model = CoreNLPParser(url=STANFORD_SERVER_URL)\n",
        "\n",
        "huggingface_model = spacy.load('en_coref_sm')\n",
        "\n",
        "try:\n",
        "    stanford_model.api_call('This is a dummy text.', properties={'annotators': 'coref'})\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUOA07zzCXJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt5TabSk6OKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.attrs import IS_TITLE\n",
        "import time\n",
        "from word2number import w2n                    \n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('csv_articles')\n",
        "\n",
        "\n",
        "article_=[]\n",
        "#sent_articles_=[]\n",
        "POS_ = []\n",
        "    \n",
        "\n",
        "# Seek of tags\n",
        "\n",
        "def find_pos(POS_, tag_):\n",
        "    \n",
        "    return [x for i, x in enumerate(POS_) if POS_[i][3]==';']\n",
        "  \n",
        "#Transform to sentenses\n",
        "\n",
        "def transform_text(df_):\n",
        "    sent_articles_=[]\n",
        "    df=df_.dropna()\n",
        "    for j in df.index:\n",
        "        doc = nlp(df.text[j])\n",
        "        #print(j)\n",
        "        for sent in doc.sents: sent_articles_.append(sent)\n",
        "    return sent_articles_\n",
        "\n",
        "#create_POS  \n",
        "def create_POS (df):\n",
        "    df=df.dropna()\n",
        "    for j in df.index:\n",
        "        doc = nlp(df.text[j])\n",
        "        #print(j)\n",
        "        for i, sent in enumerate(doc.sents): \n",
        "            sent_articles_.append(sent)\n",
        "            for token in sent:\n",
        "                POS_.append((j, i, token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop))  \n",
        "    return POS_ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFbHesIvGsiF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Transform words to numerics\n",
        "def num_sent(doc_):\n",
        "\n",
        "    row_=''\n",
        "    text=''\n",
        "    first=doc_[0]\n",
        "    buffer1_=[]\n",
        "        \n",
        "    for token in doc_:\n",
        "        try:\n",
        "            if (token.tag_=='CD') and (first.tag_=='CD'):\n",
        "                  if len(row_)<3:\n",
        "                     row_ += str(first.lemma_) +' '+ str(token.lemma_)+' '\n",
        "                  else:\n",
        "                     row_ += str(token.lemma_) + ' ' \n",
        "                  \n",
        "                  #buffer_=[]\n",
        "                  \n",
        "                  first= token\n",
        "                  #text += str(w2n.word_to_num(row_)) +' '\n",
        "                  \n",
        "            elif (token.tag_=='CD'):\n",
        "                      try: \n",
        "                          buffer1_.append(token.lemma_)\n",
        "                          first= token\n",
        "                          #text += str(w2n.word_to_num(token.lemma_)) +' '\n",
        "                           \n",
        "                      except ValueError:\n",
        "                          \n",
        "                          first= token \n",
        "                          text += token.text +' '\n",
        "            else: \n",
        "                if len(buffer1_)>0 and token.tag_!='CD' and len(row_)>0:\n",
        "                    text += str(w2n.word_to_num(row_)) +' '+ token.text+' '\n",
        "                    buffer1_=[]\n",
        "                    \n",
        "                    row_= ''\n",
        "                elif (len(buffer1_)>0) and (token.tag_!='CD')  and (len(row_) == 0): \n",
        "                    text += str(w2n.word_to_num(buffer1_[0])) +' '+ token.text+' '\n",
        "                    first= token\n",
        "                    buffer1_=[]\n",
        "                    \n",
        "                else:\n",
        "                    first= token\n",
        "                    text += token.text +' '\n",
        "        except ValueError:\n",
        "                    \n",
        "                    text += token.text +' '\n",
        "    return text\n",
        "\n",
        "#Pipeline for text with transform words to numerics\n",
        "\n",
        "def pipe_(df,i):\n",
        "    #for example  df[0:1]\n",
        "    df_=df[0:i]\n",
        "    #print(df_)\n",
        "    articles_= transform_text(df_)\n",
        "    print(articles_)\n",
        "    for i in range(0,articles_.__len__()):\n",
        "        #sent_articles_[i][1]\n",
        "        doc_=nlp(str(articles_[i]))\n",
        "        print(i)\n",
        "        try: \n",
        "            print(num_sent(doc_))\n",
        "        except ValueError:\n",
        "            print(doc_)\n",
        "            \n",
        "def depend_(sent):\n",
        "  doc=nlp(sent)\n",
        "  sub_toks = [tok for tok in doc if (tok.dep_ == \"nsubj\") ]\n",
        "  verb_toks = [tok for tok in doc if (tok.dep_ == \"ROOT\") ]\n",
        "  obj_toks = [tok for tok in doc if (tok.dep_ == \"dobj\") ]\n",
        "  print(sub_toks) \n",
        "  print(verb_toks) \n",
        "  print(obj_toks)             "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1fuAIyA6R6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "# Add css styles and js \n",
        "display(HTML(open('visualization/highlight.css').read()))\n",
        "display(HTML(open('visualization/highlight.js').read()))\n",
        "\n",
        "def out_coref(row,i):\n",
        "    try:\n",
        "       data = stanford_model.api_call(row, properties={'annotators': 'coref'})\n",
        "       html = visualization.render(data, stanford=True, jupyter=False)\n",
        "       rows.append({'sample_idx': i, \n",
        "                 'model': 'Stanford',\n",
        "                 'annotation': html})\n",
        "    \n",
        "     \n",
        "       data = huggingface_model(row)\n",
        "       html = visualization.render(data, huggingface=True, jupyter=False)\n",
        "       rows.append({'sample_idx': i, 'model': 'NeuroCoref',\n",
        "                 'annotation': html})\n",
        "            \n",
        "        \n",
        "    except:\n",
        "       print('Not text')\n",
        "       \n",
        "        \n",
        "        \n",
        "rows = []\n",
        "\n",
        "#Get first sentense from df df[0:1]\n",
        "\n",
        "sent_articles_=transform_text(df[0:2])\n",
        "\n",
        "for i in range(len(sent_articles_)):\n",
        "    row =str(sent_articles_[i])\n",
        "    out_coref(row,i)\n",
        "    \n",
        "    \n",
        "df_all = pd.DataFrame(rows).groupby(['sample_idx','model']).agg(lambda x: x)\n",
        "s = df_all.style.set_properties(**{'text-align': 'left'})\n",
        "\n",
        "display(HTML(s.render(justify='left')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-kKTtbO-WRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipe_(df,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmHbLnziQziH",
        "colab_type": "text"
      },
      "source": [
        "# **Identify sentance subject. Identify object. Identify predicate. Determine and tag if object is Direct, Indirect, or Object of a Preposition •Identify adjectives and adverbs, and mark which nouns and verbs**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BXq8fePKpts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def depend_(sent): # Identify sentance subject. Identify object. Identify predicate\n",
        "  doc=nlp(sent)\n",
        "  sub_toks = [tok for tok in doc if (tok.dep_ == \"nsubj\") ]\n",
        "  verb_toks = [tok for tok in doc if (tok.dep_ == \"ROOT\") ]\n",
        "  obj_toks = [tok for tok in doc if (tok.dep_ == \"dobj\") ]\n",
        "  print(\"Subject: \", sub_toks) \n",
        "  print(\"ROOT: \", verb_toks) \n",
        "  print(\"Object: \", obj_toks) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zvnac_uRMYdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(sent_articles_[0])\n",
        "\n",
        "depend_(str(sent_articles_[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0TnwSpAOtEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals\n",
        "# Load Library files\n",
        "import textacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "SUBJ = [\"nsubj\",\"nsubjpass\"] \n",
        "VERB = [\"ROOT\"] \n",
        "OBJ = [\"dobj\", \"pobj\", \"dobj\"] \n",
        "text = nlp(str(sent_articles_[0]))\n",
        "sub_toks = [tok for tok in text if (tok.dep_ in SUBJ) ]\n",
        "obj_toks = [tok for tok in text if (tok.dep_ in OBJ) ]\n",
        "vrb_toks = [tok for tok in text if (tok.dep_ in VERB) ]\n",
        "text_ext = list(textacy.extract.subject_verb_object_triples(text))\n",
        "\n",
        "print(sent_articles_[0])\n",
        "print(\"Subjects:\", sub_toks)\n",
        "print(\"VERB :\", vrb_toks)\n",
        "print(\"OBJECT(s):\", obj_toks)\n",
        "print (\"Extract SUB-VERB-OBJ:\", text_ext)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l65YzzwHTDwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDX1umYLO_c9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}